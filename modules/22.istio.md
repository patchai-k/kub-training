# Istio

## Module Objectives

1. Configuring & installing Istio
1. Deploying a microservice with an Istio sidecar
1. Monitoring and tracing
1. Traffic Shifting
1. Fault Injection
1. Retries
1. Control Egress Traffic

---

## Configure & Install Istio

In this exercises you will deploy Istio onto the GKE cluster.

1. Delete everything from the default namespace to ensure you have a clean state.

    ```shell
    kubectl delete deployment --all
    kubectl delete svc --all
    kubectl delete statefulset --all
    kubectl delete hpa --all
    ```

1. Grant cluster admin permissions to the current user:

    ```shell
    kubectl create clusterrolebinding cluster-admin-binding \
      --clusterrole=cluster-admin \
      --user="$(gcloud config get-value core/account)"
    ```

    > Note: You need these permissions to create the necessary role based access control (RBAC) rules for Istio

1. Download the Istio release.

    ```shell
    cd $HOME
    curl -L https://git.io/getLatestIstio | ISTIO_VERSION=1.3.4 sh -
    cd istio-1.3.4
    ```

1. Add the `istioctl` client to your PATH, we can do this by adding the following line to the `~/.bashrc` file and then sourcing it:

    ```shell
    export PATH=$HOME/istio-1.3.4/bin:$PATH
    ```

Then *source* ~/.bashrc:

    ```shell
    source ~/.bashrc
    ```

1. Install the Istio Custom Resource Definitions (CRDs):

    From the top-level istio directory, type:

    ```shell
    for i in install/kubernetes/helm/istio-init/files/crd*yaml; do kubectl apply -f $i; done
    ```

1. Install Istio's core components:

    ```shell
    kubectl apply -f install/kubernetes/istio-demo-auth.yaml
    ```

    This does the following:

    * Creates the `istio-system` namespace

    * Deploys the core Istio components:

        * `Istio-Pilot` is responsible for service discovery and for configuring the Envoy sidecar proxies in an Istio service mesh

        * The Mixer components `Istio-Policy` and `Istio-Telemetry` enforce usage policies and gather telemetry data across the service mesh

        * `Istio-Ingressgateway` provides an ingress point for traffic from outside the cluster

        * `Istio-Citadel` automates key and certificate management for Istio

    * Deploys plugins for metrics, logs, and tracing

    * Enables mutual TLS authentication between Envoy sidecars

    * Deploys prometheus and grafana add-ons for metrics gathering and graphing.

1. Verify that Istio is correctly installed.

    ```shell
    kubectl get service -n istio-system
    kubectl get pods -n istio-system
    ```

    Wait until all pods are in the `Running` or `Completed` state.

Now you are ready to deploy the sample application to the Istio cluster. We will be working in the [sample-app](../../sample-app) folder for most part in this section.

## Deploying a microservice with an Istio sidecar

1. Change your directory to the location of your *sample-app* and *sample-app.yaml* and
inject the istio sidecar container into the sample app:

    ```shell
    istioctl kube-inject -f sample-app.yaml  > sample-app-istio.yaml
    ```

    Inspect the generated file. Alternatively you can label a namespace
    with `istio-injection=enabled`
    (kubectl label namespace <namespace> istio-injection=enabled)
    for automatic sidecar injection within a namespace.

1. Deploy the sample app.

    ```shell
    kubectl create secret generic mysql --from-literal=password=very-secret-password # setup password for mysql and apps
    kubectl apply -f sample-app-istio.yaml
    ```

1. Create an Istio Gateway as `istio-gateway.yaml` and apply the changes.

    ```yaml
    apiVersion: networking.istio.io/v1alpha3
    kind: Gateway
    metadata:
      name: acme-gateway
    spec:
      selector:
        istio: ingressgateway # Use Istio default controller
      servers:
      - port:
          number: 80
          name: http
          protocol: HTTP
        hosts:
          - "*"
    ```

1. Create a VirtualService for the frontend as `frontend-vs.yaml` and apply the changes.

    ```yaml
    apiVersion: networking.istio.io/v1alpha3
    kind: VirtualService
    metadata:
      name: frontend-vs
    spec:
      hosts:
      - "*"
      gateways:
      - acme-gateway
      http:
      - match:
        - uri:
            exact: /
        - uri:
            exact: /add-note
        - uri:
            exact: /healthz
        route:
        - destination:
            host: frontend
            port:
              number: 80
    ```

1. Check that the Gateway is created.

    ```shell
    kubectl get gateway
    ```
    Output:
    ```
    NAME            AGE
    acme-gateway   3m
    ```

1. Get Ingress IP info (from the load balancer).

    ```shell
    export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
    export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')
    export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].port}')
    export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT
    echo GATEWAY_URL: $GATEWAY_URL
    ```

    > Note: You can add this to your `~/.bashrc` file to automatically load on startup of new shell sesssions.

1. Now the app should be reachable through the Istio gateway on `$GATEWAY_URL`.

You can write notes and save them in the database. But you don't see a majority of the information about the GCE instance. This is because the app gets this info from the `metadata.google.internal` server, which is not part of the Istio service mesh.

## Monitoring and Tracing

1. Set up a tunnel to Grafana.

    ```
    kubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath='{.items[0].metadata.name}') 3000:3000 &
    ```

1. Open the `acme` app in a web browser and send a couple of requests.

1. Open Web Preview at port `3000`. You should see the Grafana interface.

1. In the left panel click on `Dashboards -> Manage` and then select the `istio` folder. You should see a lot of dashboards. Let's check a couple of them.

    * `Istio Mesh Dashboard` - Displays the overall volume of the requests as well as the number of failed requests and tracks request latency. Refresh the frontend page several times, add a couple of notes and make sure you see a spike in global request volume graph and changes in overall request statistics.
    * `Istio Service Dashboard` - Contains more detail information about request statistics. You can use this dashboard to see the request statistics per service.
    * `Istio Workload Dashboard` - Gives details about metrics for each workload and then inbound workloads (workloads that are sending request to this workload) and outbound services (services to which this workload send requests) for that workload.

1. Set up a tunnel to Kiali.

    ```shell
    kubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=kiali -o jsonpath='{.items[0].metadata.name}') 5000:20001
    ```

1. Open web preview at port `5000` and append `kiali/console` to the path.

Use the username *admin* and password *admin*.

Click *Overview* (top left), find the *default* namespace box, and
click in the lower left corner is an icon that looks like a connected
group of circles (hovering over the icon will show *Go to graph*).
You can then select different types of graphs, etc.

1. Setup access to the Jaeger dashboard by using port-forwarding.

    ```shell
    kubectl port-forward -n istio-system $(kubectl get pod -n istio-system -l app=jaeger -o jsonpath='{.items[0].metadata.name}') 16686:16686 &
    ```

1. Open web preview at port `16686`, you should see the Jaeger interface.

1. Open the app and add some notes.

1. In the Jaegger UI select `Service=acme.default`, `Operation=frontend.default.svc.cluster.local:80/add-note` and click "Find Traces".

1. Make sure that in the Jaeger UI you see all requests that you've just sent. Open one of the requests. You should see all sub-requests that were sent in the context of the main request (including the request to the backend and the request to the Istio internal components).

You can zoom in on the traces by clicking on them to expand them.

You can create Services if you want to permanently expose Grafana, ServiceGraph and Jager for managing and monitoring your service mesh.

## Traffic Shifting

Let's now see how Istio can help us to add new features to our application. Let's imagine that we want to add a new feature to the app and test it on a small percent of our users (this is called 'Canary deployment').

1. In the `sample-app` folder open `main.go` file.

1. At line 60 find `version` constant and change it from `1.0.0` to `1.0.1`

1. Now rebuild the app image with a new version tag and push it to the Google Container Registry. (These commands should be executed in the `sample-app` folder)

    ```shell
    export IMAGE_V1=gcr.io/$PROJECT_ID/sample-k8s-app:1.0.1
    docker build . -t $IMAGE_V1
    docker push $IMAGE_V1
    ```

1. Edit `sample-app.yaml`.

    1. Duplicate the `backend` deployment section.

    1. Keep the name for the first Deployment (`backend`) and name the second one `backend-v1`.

    1. Modify the first Deployment, add `version: v0` label to the `spec -> selectors -> matchLabels` and to the `spec -> templates -> metadata -> labels` elements.

    1. Modify the second Deployment, but this time use `version: v1` label instead.

1. Change the second Deployment image. Change the image tag from `1.0.0` to `1.0.1`

1. Configure the default namespace for automatic sidecar injection.

    ```shell
    kubectl label namespace default istio-injection=enabled
    ```

1. Delete the old sample application.

    ```shell
    kubectl delete -f sample-app.yaml
    ```

1. Apply changes as usual (without `istioctl kube-inject`).

    ```shell
    kubectl apply -f sample-app.yaml
    ```

1. Create a destination rule for the backend Service as `backend-dr.yaml` and apply the changes.

    ```yaml
    apiVersion: networking.istio.io/v1alpha3
    kind: DestinationRule
    metadata:
      name: backend-dr
    spec:
      host: backend
      trafficPolicy:
        tls:
          mode: ISTIO_MUTUAL
      subsets:
      - name: v0
        labels:
          version: v0
      - name: v1
        labels:
          version: v1
    ```

1. Create a VirtualService for the backend as `backend-vs.yaml` and apply the changes.

    ```yaml
    apiVersion: networking.istio.io/v1alpha3
    kind: VirtualService
    metadata:
      name: backend-vs
    spec:
      hosts:
      - backend
      http:
      - route:
        - destination:
            host: backend
            subset: v0
          weight: 50
        - destination:
            host: backend
            subset: v1
          weight: 50
    ```

1. Open the app and refresh the page several times. You should see `1.0.0` and `1.0.1` backend version each in 50% of the cases.

Another option to observe is using curl:
```
while true; do curl -s http://$GATEWAY_URL|grep -A1 Version; sleep 1;  done
```

1. Change the weight of v0 to 90% and v1 to 10% and see the backend used.
Try a few other distributions.

## Fault Injection

One of the most difficult aspects of testing microservice applications is verifying that the application is resilient to failures. Each service should not assume that all its dependencies are available 100% of the time, instead it should be ready to handle any unexpected failure.

Usually people manually shut down application instances or block application ports in order to simulate failures, Istio provides us with a much better way: Fault Injection.

1. Modify `backend-vs.yaml` and add the following lines to `spec -> http[0]` (if you simply append them to the end of the file (including the spaces) it should work fine). Apply the changes.

    ```yaml
        fault:
          delay:
            fixedDelay: 3s
            percent: 50
    ```

1. Open the app and verify that in 50% of the times it should take 3 seconds to complete the request.

In a similar way you can inject not only delays, but also failures.

## Retries

Now let's inject a native failure to the backend application to demonstrate how Istio can help make microservices more resilient.

1. Modify `sample-app.yaml` and add `-fail-percent=50` parameter to the `backend` Deployment `command` property (leaving the second `backend_v1` deployment untouched) then apply the changes.

1. Delete the fault definition added from the previous exercise from the `backend-vs.yaml` file then apply the changes.

1. Observe that the application is failing 50% of the times (Failures should happen only if frontend connects to the `1.0.0` version of the backend).

1. Add retries to the `spec -> http[0]` section of the `backend-vs.yaml` then apply the changes.

    ```yaml
        retries:
          attempts: 3
          perTryTimeout: 2s
    ```
1. Test the app. You should no longer see any failures. Each failed request now retries up to 3 times.

<!-- To do: Now let's demonstrate how we can automatically remove a failing app from the system (apply Circuit Breaking pattern). -->

1. Disable sidecar injection from the default namespace by deleting the label `istio-injection`

    ```
    kubectl label ns default istio-injection-
    ```

## **IMPORTANT CLEANUP**
Run the following to cleanup your environment

```shell
~/kubernetes-training/starting-points/cleanup.sh istio
```

<!-- TODO: Uninstall Istio -->
